{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson 11 (Computer Vision).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMRf7fBlZgWcmDmnmgP0pSB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pttrilok/practical_deep_learning/blob/master/Lesson%2011%20(Computer%20Vision).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQZF7LPx5o1I",
        "colab_type": "text"
      },
      "source": [
        "In this module we are going to learn about Image captioning.\n",
        "\n",
        "Objective\n",
        "\n",
        "Concepts\n",
        "\n",
        "Overview\n",
        "\n",
        "Implementation\n",
        "\n",
        "Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Os8FKsCg0T",
        "colab_type": "text"
      },
      "source": [
        "# Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VuT-1SKClfZ",
        "colab_type": "text"
      },
      "source": [
        "To build a model that can generate a descriptive caption for an image we provide it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxZfHyF4Crsi",
        "colab_type": "text"
      },
      "source": [
        "# Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cYr_XjlCza-",
        "colab_type": "text"
      },
      "source": [
        "1.) Image captioning.\n",
        "\n",
        "2.) Encoder-Decoder architecture. Typically, a model that generates sequences will use an Encoder to encode the input into a fixed form and a Decoder to decode it, word by word, into a sequence.\n",
        "\n",
        "3.) Attention. The use of Attention networks is widespread in deep learning, and with good reason. This is a way for a model to choose only those parts of the encoding that it thinks is relevant to the task at hand. The same mechanism you see employed here can be used in any model where the Encoder's output has multiple points in space or time. In image captioning, you consider some pixels more important than others. In sequence to sequence tasks like machine translation, you consider some words more important than others.\n",
        "\n",
        "4.) Transfer Learning. This is when you borrow from an existing model by using parts of it in a new model. This is almost always better than training a new model from scratch (i.e., knowing nothing). As you will see, you can always fine-tune this second-hand knowledge to the specific task at hand. Using pretrained word embeddings is a dumb but valid example. For our image captioning problem, we will use a pretrained Encoder, and then fine-tune it as needed.\n",
        "\n",
        "5.) Beam Search. This is where you don't let your Decoder be lazy and simply choose the words with the best score at each decode-step. Beam Search is useful for any language modeling problem because it finds the most optimal sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK3G2Mz7DGKS",
        "colab_type": "text"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trQBLCZQDfrg",
        "colab_type": "text"
      },
      "source": [
        "***Encoder***\n",
        "\n",
        "The Encoder encodes the input image with 3 color channels into a smaller image with \"learned\" channels.\n",
        "\n",
        "This smaller encoded image is a summary representation of all that's useful in the original image.\n",
        "\n",
        "Since we want to encode images, we use Convolutional Neural Networks (CNNs).\n",
        "\n",
        "We don't need to train an encoder from scratch. Why? Because there are already CNNs trained to represent images.\n",
        "\n",
        "For years, people have been building models that are extraordinarily good at classifying an image into one of a thousand categories. It stands to reason that these models capture the essence of an image very well.\n",
        "\n",
        "I have chosen to use the 101 layered Residual Network trained on the ImageNet classification task, already available in PyTorch. As stated earlier, this is an example of Transfer Learning. You have the option of fine-tuning it to improve performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c4cUwGPDsHp",
        "colab_type": "text"
      },
      "source": [
        "These models progressively create smaller and smaller representations of the original image, and each subsequent representation is more \"learned\", with a greater number of channels. The final encoding produced by our ResNet-101 encoder has a size of 14x14 with 2048 channels, i.e., a 2048, 14, 14 size tensor.\n",
        "\n",
        "I encourage you to experiment with other pre-trained architectures. The paper uses a VGGnet, also pretrained on ImageNet, but without fine-tuning. Either way, modifications are necessary. Since the last layer or two of these models are linear layers coupled with softmax activation for classification, we strip them away.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT1K6g1qD3hO",
        "colab_type": "text"
      },
      "source": [
        "***Decoder***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUF2Nc1MesGC",
        "colab_type": "text"
      },
      "source": [
        "The Decoder's job is to look at the encoded image and generate a caption word by word.\n",
        "\n",
        "Since it's generating a sequence, it would need to be a Recurrent Neural Network (RNN). We will use an LSTM.\n",
        "\n",
        "In a typical setting without Attention, you could simply average the encoded image across all pixels. You could then feed this, with or without a linear transformation, into the Decoder as its first hidden state and generate the caption. Each predicted word is used to generate the next word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc06GeGrex4m",
        "colab_type": "text"
      },
      "source": [
        "In a setting with Attention, we want the Decoder to be able to look at different parts of the image at different points in the sequence. For example, while generating the word football in a man holds a football, the Decoder would know to focus on – you guessed it – the football!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdn4NKO2e2PH",
        "colab_type": "text"
      },
      "source": [
        "Instead of the simple average, we use the weighted average across all pixels, with the weights of the important pixels being greater. This weighted representation of the image can be concatenated with the previously generated word at each step to generate the next word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lekWROOWe7px",
        "colab_type": "text"
      },
      "source": [
        "***Attention***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1faC4dsXe_G8",
        "colab_type": "text"
      },
      "source": [
        "The Attention network computes these weights.\n",
        "\n",
        "Intuitively, how would you estimate the importance of a certain part of an image? You would need to be aware of the sequence you have generated so far, so you can look at the image and decide what needs describing next. For example, after you mention a man, it is logical to declare that he is holding a football.\n",
        "\n",
        "This is exactly what the Attention mechanism does – it considers the sequence generated thus far, and attends to the part of the image that needs describing next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEIQijcdfDwp",
        "colab_type": "text"
      },
      "source": [
        "We will use soft Attention, where the weights of the pixels add up to 1. If there are P pixels in our encoded image, then at each timestep t –"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFLqOvyAfJPT",
        "colab_type": "text"
      },
      "source": [
        "You could interpret this entire process as computing the probability that a pixel is the place to look to generate the next word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m_gQ5zNfOsf",
        "colab_type": "text"
      },
      "source": [
        "***Putting it all together***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dABLq4sefUK3",
        "colab_type": "text"
      },
      "source": [
        "It might be clear by now what our combined network looks like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF2aSOr0fVis",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "*   Once the Encoder generates the encoded image, we transform the encoding to create the initial hidden state h (and cell state C) for the LSTM Decoder.\n",
        "*   At each decode step,\n",
        "the encoded image and the previous hidden state is used to generate weights for each pixel in the Attention network.\n",
        "the previously generated word and the weighted average of the encoding are fed to the LSTM Decoder to generate the next word.\n",
        "\n",
        "\n",
        "At each decode step,\n",
        "the encoded image and the previous hidden state is used to generate weights for each pixel in the Attention network.\n",
        "the previously generated word and the weighted average of the encoding are fed to the LSTM Decoder to generate the next word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK4Gnnerfmsv",
        "colab_type": "text"
      },
      "source": [
        "***Beam Search***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldt7Zqpcf779",
        "colab_type": "text"
      },
      "source": [
        "We use a linear layer to transform the Decoder's output into a score for each word in the vocabulary.\n",
        "\n",
        "The straightforward – and greedy – option would be to choose the word with the highest score and use it to predict the next word. But this is not optimal because the rest of the sequence hinges on that first word you choose. If that choice isn't the best, everything that follows is sub-optimal. And it's not just the first word – each word in the sequence has consequences for the ones that succeed it.\n",
        "\n",
        "It might very well happen that if you'd chosen the third best word at that first step, and the second best word at the second step, and so on... that would be the best sequence you could generate.\n",
        "\n",
        "It would be best if we could somehow not decide until we've finished decoding completely, and choose the sequence that has the highest overall score from a basket of candidate sequences.\n",
        "\n",
        "Beam Search does exactly this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhm8kjHhf-_q",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   At the first decode step, consider the top k candidates.\n",
        "\n",
        "*   Generate k second words for each of these k first words\n",
        "*   Choose the top k [first word, second word] combinations considering additive scores.\n",
        "\n",
        "\n",
        "*   For each of these k second words, choose k third words, choose the top k [first word, second word, third word] combinations.\n",
        "\n",
        "*   Repeat at each decode step.\n",
        "*   After k sequences terminate, choose the sequence with the best overall score.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLnHAQp9gbFx",
        "colab_type": "text"
      },
      "source": [
        "As you can see, some sequences (striked out) may fail early, as they don't make it to the top k at the next step. Once k sequences (underlined) generate the <end> token, we choose the one with the highest score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwprWJB0glMn",
        "colab_type": "text"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Jes1bNgr8k",
        "colab_type": "text"
      },
      "source": [
        "The sections below briefly describe the implementation.\n",
        "\n",
        "They are meant to provide some context, but details are best understood directly from the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh_1AKFug6pl",
        "colab_type": "text"
      },
      "source": [
        "***Dataset***\n",
        "\n",
        "\n",
        "Flicker8k\n",
        "Flicker30k\n",
        "MSCOCO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJiFyW6ghR_1",
        "colab_type": "text"
      },
      "source": [
        "***Inputs to model***\n",
        "\n",
        "We will need three inputs.\n",
        "\n",
        "***1.) Images***\n",
        "Since we're using a pretrained Encoder, we would need to process the images into the form this pretrained Encoder is accustomed to.\n",
        "\n",
        "Pretrained ImageNet models available as part of PyTorch's torchvision module. This page details the preprocessing or transformation we need to perform – pixel values must be in the range [0,1] and we must then normalize the image by the mean and standard deviation of the ImageNet images' RGB channels.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "```\n",
        "\n",
        "Also, PyTorch follows the NCHW convention, which means the channels dimension (C) must precede the size dimensions.\n",
        "\n",
        "We will resize all MSCOCO images to 256x256 for uniformity.\n",
        "\n",
        "Therefore, images fed to the model must be a Float tensor of dimension N, 3, 256, 256, and must be normalized by the aforesaid mean and standard deviation. N is the batch size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Z5O_EDh3xl",
        "colab_type": "text"
      },
      "source": [
        "***Captions***\n",
        "Captions are both the target and the inputs of the Decoder as each word is used to generate the next word.\n",
        "\n",
        "To generate the first word, however, we need a zeroth word, `<start>.`\n",
        "\n",
        "At the last word, we should predict `<end> `the Decoder must learn to predict the end of a caption. This is necessary because we need to know when to stop decoding during inference.\n",
        "\n",
        "`<start> a man holds a football <end>`\n",
        "\n",
        "Since we pass the captions around as fixed size Tensors, we need to pad captions (which are naturally of varying length) to the same length with <pad> tokens.\n",
        "\n",
        "`<start>` a man holds a football `<end> <pad> <pad> <pad>`....\n",
        "\n",
        "Furthermore, we create a word_map which is an index mapping for each word in the corpus, including the `<start>,<end>, and <pad>` tokens. PyTorch, like other libraries, needs words encoded as indices to look up embeddings for them or to identify their place in the predicted word scores.\n",
        "\n",
        "9876 1 5 120 1 5406 9877 9878 9878 9878....\n",
        "\n",
        "Therefore, captions fed to the model must be an Int tensor of dimension N, L where L is the padded length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzOrkVE2ib3r",
        "colab_type": "text"
      },
      "source": [
        "***Caption Lengths***\n",
        "Since the captions are padded, we would need to keep track of the lengths of each caption. This is the actual length + 2 (for the <start> and <end> tokens).\n",
        "\n",
        "Caption lengths are also important because you can build dynamic graphs with PyTorch. We only process a sequence upto its length and don't waste compute on the <pad>s.\n",
        "\n",
        "Therefore, caption lengths fed to the model must be an Int tensor of dimension N."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rionPBq55lek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}